# SVM This is a reimplementation of 'The perfect solution for detecting sarcasm in tweets# not' Lieberecht et al (2013)
#!/usr/bin/python

import sys

from sklearn.metrics.pairwise import cosine_similarity;
import warnings
warnings.filterwarnings('ignore')

dict = {}
import pickle
with open('./google.pkl_filtered', 'rb') as infile:
    model = pickle.load( infile);
import re

f = open('/data/aa1/PhD_Sem8/AAAI17Demo/booksnippets_dataset.emnlpvocab','r')

maxindex = 0

for line in f:
	words = line.split('\t')
	word = words[0]
	index = int(words[1])
	dict[word] = index
	if maxindex < index:
		maxindex = index
	
def f3(seq):
   # Not order preserving
   keys = {}
   for e in seq:
       keys[e] = 1
   return list(keys.keys())

def get_distance(sentence):
    dic1 = {}
    words = sentence.split(' ')
    index = 0
    for word in words:
        dic1[word] = index
        index += 1
    return dic1


def get_wv_features(line,wvindex):
    line = line.strip().lower()    
    contents = line.split('\t')
    sentence = contents[0]
    words = re.findall(r"[\w']+|[.,!?;]",sentence)
        
    max_same = []
    min_same = []
    max_weighted = []
    min_weighted = []

    word_list = f3(sentence.split(' '))

    dic1 = get_distance(sentence)
    
    

    for e1 in word_list:
        if e1 not in model.keys():
            continue
        output = e1+"\t"
        same_list = []
        weighted_list = []
        for e2 in word_list:
            
            if e2 not in model.keys():
                continue
                
            if e1 == e2:
         #   output += '0\t'
                continue
            
            weighted_list.append(cosine_similarity( model[e1], model[e2])[ 0][ 0]/(dic1[e1] - dic1[e2])*2)
            same_list.append(cosine_similarity( model[e1], model[e2])[ 0][ 0])
                
        if len(same_list) != 0:
            max_same.append (max(same_list))
            min_same.append(min(same_list))
        else:
            min_same.append(0)           
            max_same.append(0)
        
        if len(weighted_list) != 0:
            min_weighted.append(min(weighted_list))
            max_weighted.append(max(weighted_list))
        else:
            min_weighted.append(0)
            max_weighted.append(0)
    #print(min_same)  
          
   # min_same.append(0)
   # max_same.append(0)
   # min_weighted.append(0)
   # max_weighted.append(0)

    if len(max_same) == 0:
        a = 0
        b = 0
    else:
        a = max(max_same)
        b = min(max_same)
        
    if len(min_same) == 0:
        c = 0
        d = 0
    else:
        c = max(min_same)
        d = min(min_same)
        
    if len(max_weighted) == 0:
        e = 0
        fk = 0
    else:
        e = max(max_weighted)
        fk = min(max_weighted)
        
    if len(min_weighted) == 0:
        g = 0
        h = 0
    else:
        g = max(min_weighted)
        h = min(min_weighted)
        
   
    
    stt = (str(wvindex)+':'+str(a)+' '+str(int(wvindex)+1)+':'+str(b)+
           ' '+str(int(wvindex)+2)+':'+str(c)+' '+str(int(wvindex)+3)+':'+str(d) +
          ' '+str(int(wvindex)+4)+':'+str(e)+' '+str(int(wvindex)+5)+':'+str(fk)
         +' '+str(int(wvindex)+6)+':'+str(g)+' '+str(int(wvindex)+7)+':'+str(h)
        )
   # print(stt)
    return(stt.strip())
    

f_o1 = open('/data/aa1/PhD_Sem8/AAAI17Demo/emnlptest','w')
qid = 0

word_count = {}
rev_dict = {}
index = 1

def bigramgen(input):
	input = str(input.strip())
	input = input.lower()
	words = input.split(' ')
	prev_word = 'init'
	output = ''
	for word in words:
		output+= prev_word+word+' '
		prev_word = word

	prev_word = 'init'
	prev_prev_word = 'init'
	for word in words:
		output+= prev_prev_word+prev_word+word+' '
		prev_prev_word = prev_word
		prev_word = word
	output += input.strip()
	return output

line = sys.argv[1]

s_line = ''

pos_score = 0
neg_score = 0


#print(contents)
word_ids = [1]
dialogue = line



words = re.findall(r"[\w']+|[.,!?;]",dialogue)
	
first_word = words[0]
speaker = first_word+':'

stitched = ''
for word in words:
	stitched+= word+' '
	
stitched = stitched.strip()
stitched = bigramgen(stitched)
words = stitched.split(' ')

for word in words:
	if word in dict:
		index = dict[word]
		word_ids.append(index)

label = '-1'
	
word_ids = list(set(word_ids))
word_ids.sort()
s_line = label+' ' 
#print(word_ids)
for id in word_ids:
	s_line += str(id)+':1 '

s_line += get_wv_features(dialogue, maxindex)
		
s_line += '# '+ stitched
s_line = s_line.strip()
f_o1.write(s_line+'\n')

# SVM This is a reimplementation of 'The perfect solution for detecting sarcasm in tweets# not' Lieberecht et al (2013)
#!/usr/bin/python

import sys

from sklearn.metrics.pairwise import cosine_similarity;
import warnings
warnings.filterwarnings('ignore')

import pickle
with open('/data/aa1/PhD_Sem8/AAAI17Demo/glove.840B.300d.pkl_filtered', 'rb') as infile:
    model = pickle.load( infile);
import re

def f3(seq):
   # Not order preserving
   keys = {}
   for e in seq:
       keys[e] = 1
   return list(keys.keys())

def get_distance(sentence):
    dic1 = {}
    words = sentence.split(' ')
    index = 0
    for word in words:
        dic1[word] = index
        index += 1
    return dic1


def get_wv_features(line,wvindex):
    line = line.strip().lower()    
    contents = line.split('\t')
    sentence = contents[0]
    words = re.findall(r"[\w']+|[.,!?;]",sentence)
        
    max_same = []
    min_same = []
    max_weighted = []
    min_weighted = []

    word_list = f3(sentence.split(' '))

    dic1 = get_distance(sentence)
    
    

    for e1 in word_list:
        if e1 not in model.keys():
            continue
        output = e1+"\t"
        same_list = []
        weighted_list = []
        for e2 in word_list:
            
            if e2 not in model.keys():
                continue
                
            if e1 == e2:
         #   output += '0\t'
                continue
            
            weighted_list.append(cosine_similarity( model[e1], model[e2])[ 0][ 0]/(dic1[e1] - dic1[e2])*2)
            same_list.append(cosine_similarity( model[e1], model[e2])[ 0][ 0])
                
        if len(same_list) != 0:
            max_same.append (max(same_list))
            min_same.append(min(same_list))
        else:
            min_same.append(0)           
            max_same.append(0)
        
        if len(weighted_list) != 0:
            min_weighted.append(min(weighted_list))
            max_weighted.append(max(weighted_list))
        else:
            min_weighted.append(0)
            max_weighted.append(0)
    #print(min_same)  
          
   # min_same.append(0)
   # max_same.append(0)
   # min_weighted.append(0)
   # max_weighted.append(0)

    if len(max_same) == 0:
        a = 0
        b = 0
    else:
        a = max(max_same)
        b = min(max_same)
        
    if len(min_same) == 0:
        c = 0
        d = 0
    else:
        c = max(min_same)
        d = min(min_same)
        
    if len(max_weighted) == 0:
        e = 0
        fk = 0
    else:
        e = max(max_weighted)
        fk = min(max_weighted)
        
    if len(min_weighted) == 0:
        g = 0
        h = 0
    else:
        g = max(min_weighted)
        h = min(min_weighted)
        
   
    
    stt = (str(wvindex)+':'+str(a)+' '+str(int(wvindex)+1)+':'+str(b)+
           ' '+str(int(wvindex)+2)+':'+str(c)+' '+str(int(wvindex)+3)+':'+str(d) +
          ' '+str(int(wvindex)+4)+':'+str(e)+' '+str(int(wvindex)+5)+':'+str(fk)
         +' '+str(int(wvindex)+6)+':'+str(g)+' '+str(int(wvindex)+7)+':'+str(h)
        )
   # print(stt)
    return(stt.strip())
    

f = open(sys.argv[1],'r')
f_o1 = open(sys.argv[1]+'.emnlpvocab','w')
sep = str(sys.argv[2])
if sep == 'p':
	sep = '\t'
qid = 0
dict = {}
word_count = {}
rev_dict = {}
index = 1

def bigramgen(input):
	input = str(input.strip())
	input = input.lower()
	words = input.split(' ')
	prev_word = 'init'
	output = ''
	for word in words:
		output+= prev_word+word+' '
		prev_word = word

	prev_word = 'init'
	prev_prev_word = 'init'
	for word in words:
		output+= prev_prev_word+prev_word+word+' '
		prev_prev_word = prev_word
		prev_word = word
	output += input.strip()
	return output



# main method starts
for line in f:
	contents = line.split(sep)
	if len(contents) ==2:
		dialogue = contents[0].lower()
		if len(dialogue) == 0:
			continue

		words = re.findall(r"[\w']+|[.:,!?;]",dialogue)
		first_word = words[0]

		stitched = ''
		for word in words:
			stitched+= word+' '
		
		stitched = stitched.strip()
		stitched = bigramgen(stitched)
		words = stitched.split(' ')	
		for word in words:
			if word not in dict:
				dict[word] = index
				f_o1.write(word+'\t'+str(index)+'\n')
				rev_dict[index] = word
				index += 1
				word_count[word] = 1
			else:
				word_count[word] += word_count[word] 

wvfeaturesid = index + 1
print(str(index))
findex = open(sys.argv[1] + '.featind', 'w')
findex.write(str(index))
f = open(sys.argv[1],'r')
f_o1 = open(sys.argv[1]+'.emnlpo','w')
f_o1.write('# Vocabulary size:'+str(index)+'\n')
for line in f:
	s_line = ''
	contents = line.split(sep)
	pos_score = 0
	neg_score = 0
	if "Scene" in line:
		qid +=1
		
	
	if len(contents) >=2:
		#print(contents)
		word_ids = [1]
		dialogue = contents[0].lower()
		#dialogue = dialogue + ' '+ getActions(dialogue).lower()
		if len(dialogue) == 0:
			continue
		words = re.findall(r"[\w']+|[.,!?;]",dialogue)
		
		first_word = words[0]
		speaker = first_word+':'
		
		stitched = ''
		for word in words:
			stitched+= word+' '
		
		stitched = stitched.strip()
		stitched = bigramgen(stitched)
		words = stitched.split(' ')
		
		for word in words:
			if word in dict:
				index = dict[word]
				if word_count[word] >= 3:
					word_ids.append(index)
		
		if contents[1].strip().lower() == 'sarcasm':
			label = '+1'
			
		else:
			label = '-1'
		

		word_ids = list(set(word_ids))
		word_ids.sort()
		s_line = label+' ' 
		#print(word_ids)
		for id in word_ids:
			s_line += str(id)+':1 '
		s_line += get_wv_features(dialogue, wvfeaturesid)
		
		s_line += ' # '+ stitched
		s_line = s_line.strip()
		f_o1.write(s_line+'\n')

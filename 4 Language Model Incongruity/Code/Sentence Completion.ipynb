{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "import re\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_similarity(w1,w2):\n",
    "    s_syn1 = wn.synsets(w1)[0]\n",
    "    s_syn2 = wn.synsets(w2)[0]\n",
    "    return (s_syn1.wup_similarity(s_syn2))\n",
    "    \n",
    "    \n",
    "i = get_similarity('ascend','descend')\n",
    "print(i)\n",
    "\n",
    "l1 = []\n",
    "\n",
    "f = open ('stopwords2.kk','r')\n",
    "for line in f:\n",
    "    l1.append(line.strip())\n",
    "#print ('yah'+str(len(l1)))\n",
    "\n",
    "f2 = open ('input_file.kk','r')\n",
    "for line in f2:\n",
    "    f2 = line.split('\\t')\n",
    "    id = f2[0]\n",
    "    text = f2[1]\n",
    "    label = f2[2]\n",
    "    text = text.lower()\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    #for word in words:\n",
    "     #   if word not in l1:\n",
    "      #      temp_text = text.replace(word,\"[]\")\n",
    "       #     print(temp_text)\n",
    "        \n",
    "\n",
    "    for i in range(0,len(words)):\n",
    "        if words[i].isalnum() and words[i] not in l1:\n",
    "            word = words[i]\n",
    "            left = ''\n",
    "            right = ''\n",
    "            for j in range(0,i):\n",
    "                left += words[j]+' '\n",
    "                \n",
    "            for j in range(i+1,len(words)):\n",
    "                right += words[j]+' '\n",
    "                \n",
    "            print((id+'\\t'+left+' [] '+right+'\\t'+word+'\\t'+label).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3cfd61eee592>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mget_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'resent'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'hate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-3cfd61eee592>\u001b[0m in \u001b[0;36mget_similarity\u001b[1;34m(w1, w2)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msyn1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyn1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mantonyms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0ms_syn1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0ms_syn2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_similarity(w1,w2):\n",
    "    syn1 = wn.synsets(w1)[0]\n",
    "    print(syn1.lemmas()[0].antonyms()[0].name())\n",
    "    s_syn1 = wn.synsets(w1)[0]\n",
    "    s_syn2 = wn.synsets(w2)[0]\n",
    "    print(s_syn1.wup_similarity(s_syn2))\n",
    "    \n",
    "    \n",
    "get_similarity('resent','hate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descend\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "syn1 = wn.synsets('ascend')[0]\n",
    "print(syn1.lemmas()[0].antonyms()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open ('tweets.o','r')\n",
    "d_minval = dict()\n",
    "d_predictedlabel = dict()\n",
    "d_actuallabel = dict()\n",
    "threshold = 0.22\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "for line in f:\n",
    "    words = line.strip().split('\\t')\n",
    "    id = words[0].strip()\n",
    "    label = words[1].strip()\n",
    "    d_actuallabel[id] = label.lower()\n",
    "    \n",
    "    if id not in d_minval.keys():\n",
    "        d_minval[id] = '1'\n",
    "    word1 = words[2].strip()\n",
    "    word2 = words[3].strip()\n",
    "    val = words[4].strip()\n",
    "    if words[4] == 'None' or words[4] == '-1':\n",
    "        continue\n",
    "    f_val = float(val)\n",
    "    if (float(d_minval[id]) > f_val):\n",
    "        d_minval[id] = str(f_val)\n",
    "\n",
    "for id in d_minval.keys():\n",
    "    if (float(d_minval[id]) < threshold):\n",
    "        d_predictedlabel[id] = 'sarcasm'\n",
    "    else:\n",
    "        d_predictedlabel[id] = 'not_sarcasm'\n",
    "        \n",
    "    #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "       if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "        \n",
    "precision = float(tp)/float(tp+fp)\n",
    "recall = float(tp)/float(tp+fn)\n",
    "fscore = float(2*precision*recall)/float(precision+recall)\n",
    "print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "print('Precision : ' + str(float(tp)/float(tp+fp)))\n",
    "print('Recall : '+ str(float(tp)/float(tp+fn)))\n",
    "print('Accuracy : '+ str(float(tp+tn)/float(tp+tn+fp+fn)))\n",
    "print('F-score : '+ str(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/adityaj/work/Sem8/Datasets/BookSnippets/all.stats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cf2a6f808cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0macceptable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mfk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/adityaj/work/Sem8/Datasets/BookSnippets/all.stats'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/adityaj/work/Sem8/Datasets/BookSnippets/all.stats'"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using minimum as the measure\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 5:\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        id = words[0].strip()\n",
    "        label = words[1].strip()\n",
    "        d_actuallabel[id] = label.lower()\n",
    "        \n",
    "#         if int(id) not in acceptable:\n",
    "#             continue\n",
    "            \n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "        word1 = words[2].strip()\n",
    "        word2 = words[3].strip()\n",
    "        val = words[4].strip()\n",
    "        if words[4] == 'None' or words[4] == '-1':\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "\n",
    "   \n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    #print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "    #print('Precision : ' + str(float(tp)/float(tp+fp)))\n",
    "    #print('Recall : '+ str(float(tp)/float(tp+fn)))\n",
    "    #print('Accuracy : '+ str(float(tp+tn)/float(tp+tn+fp+fn)))\n",
    "    #print('F-score : '+ str(fscore))\n",
    "    return (str(threshold)+':'+str(aprecision)+':'+str(arecall)+':'+str(fscore))\n",
    "\n",
    "acceptable = []\n",
    "fk = open('/Users/adityaj/work/Sem8/Datasets/BookSnippets/all.stats','r')\n",
    "for line in fk:\n",
    "    words = line.split('\\t')\n",
    "    fkid = int(words[0])\n",
    "    fklen = int(words[1])\n",
    "    if fklen <= 120:\n",
    "        acceptable.append(fkid)\n",
    "print(len(acceptable))\n",
    "for i in np.arange(0.1,0.5,0.01):\n",
    "    print(calc_val('tweets.o',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-77f206966f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mcalc_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iaccorpus.o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-77f206966f7b>\u001b[0m in \u001b[0;36mcalc_val\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mword1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mword2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Return features as: min value, average value, max value\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_val(file):\n",
    "    f = open (file,'r')\n",
    "    f2 = open(file+'.o','w')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    d_countval = dict()\n",
    "\n",
    "    d_sumval = dict()\n",
    "    # 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        id = words[0].strip()\n",
    "        label = words[1].strip()\n",
    "        d_actuallabel[id] = label.lower()\n",
    "\n",
    "\n",
    "        word1 = words[2].strip()\n",
    "        word2 = words[3].strip()\n",
    "        val = words[4].strip()\n",
    "        if words[4] == 'None' or words[4] == '-1':\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        \n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "            d_sumval[id] = str(f_val)\n",
    "            d_countval[id] = str(1.0)\n",
    "\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "        \n",
    "        d_sumval[id] = str(float(d_sumval[id])+f_val)\n",
    "        d_countval[id] = str(float(d_countval[id])+1.0)\n",
    "\n",
    "    for id in range(1,100000):    \n",
    "        if str(id) in d_minval.keys():\n",
    "            if d_actuallabel[str(id)].lower().startswith(\"sar\"):\n",
    "                k_label = '+1'\n",
    "            else:\n",
    "                k_label = '-1'\n",
    "            f2.write(k_label+' '+\n",
    "                 '37605:'+str(d_minval[str(id)])+' '+\n",
    "                 '37606:'+str(float(d_sumval[str(id)])/float(d_countval[str(id)]))+''\n",
    "                 +' # '+str(id)+' '+d_actuallabel[str(id)]+'\\n')\n",
    "\n",
    "\n",
    "\n",
    "calc_val('iaccorpus.o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1:0.12475768485184159:0.00027693159789531985:0.0005526364777490216\n",
      "0.11:0.19933536416505124:0.0008307947936859596:0.001654693117031416\n",
      "0.12:0.24074586910366474:0.002215452783162559:0.004390502172103307\n",
      "0.13:0.2336144608645301:0.003600110772639158:0.007090946659795339\n",
      "0.14:0.19645527554693992:0.004430905566325118:0.008666347970088334\n",
      "0.15:0.1972818098545044:0.005815563555801717:0.011298077216337257\n",
      "0.16:0.2110264931228653:0.007200221545278316:0.01392531161657621\n",
      "0.17:0.21436178418293098:0.009138742730545555:0.017530134930437275\n",
      "0.18:0.22384974482731337:0.010800332317917475:0.020606442263555423\n",
      "0.19:0.2542121296039878:0.014677374688451952:0.02775241589563186\n",
      "0.2:0.25886152145524655:0.01800055386319579:0.033660448110883534\n",
      "0.21:0.2372945177505185:0.01938521185267239:0.03584236671265642\n",
      "0.22:0.23612521410915208:0.022708391027416227:0.041432206537430426\n",
      "0.23:0.22728297781535492:0.025754638604264746:0.0462665672982661\n",
      "0.24:0.2151797428751726:0.027970091387427305:0.04950525334466524\n",
      "0.25:0.21241113196077913:0.031570202160066466:0.054970290257769415\n",
      "0.26:0.19924802805171862:0.03461644973691498:0.05898509609882247\n",
      "0.27:0.196169575194087:0.03877042370534478:0.06474485046395087\n",
      "0.28:0.19244434911546104:0.042647466075879256:0.06982177447323233\n",
      "0.29:0.18836951942130992:0.04818609803378565:0.07674129430585466\n",
      "0.3:0.18195468549004606:0.051232345610634174:0.07995269109520464\n",
      "0.31:0.18135154095673375:0.05732484076433121:0.08711333842709525\n",
      "0.32:0.1751711844940797:0.06064801993907505:0.09010110534029689\n",
      "0.33:0.17510752266579616:0.0667405150927721:0.0966455330185573\n",
      "0.34:0.16453100166946102:0.07061755746330656:0.09882074130276475\n",
      "0.35:0.160986853894147:0.07421766823594572:0.10159727206312755\n",
      "0.36:0.15544474539179065:0.07892550540016616:0.10469379155750994\n",
      "0.37:0.151068262156378:0.08418720576017724:0.10812088647820464\n",
      "0.38:0.14437442048979893:0.0875103849349211:0.10897015083569242\n",
      "0.39:0.13977935768222116:0.09138742730545556:0.1105182640288762\n",
      "0.4:0.13402485962778787:0.09637219606757133:0.11212183255550227\n",
      "0.41:0.1300418004559992:0.10135696482968706:0.11392145657246698\n",
      "0.42:0.12644927031250855:0.10661866518969815:0.11569032338902183\n",
      "0.43:0.12222050485460653:0.1102187759623373:0.11590979283043504\n",
      "0.44:0.11971546227675467:0.11464968152866242:0.11712782371325711\n",
      "0.45:0.11761624145928921:0.11908058709498753:0.11834388462595147\n",
      "0.46:0.11486196085370316:0.12489615065078925:0.11966908378455732\n",
      "0.47:0.11130499556011218:0.12905012461921905:0.11952250933582809\n",
      "0.48:0.10684609220106706:0.13209637219606757:0.11813706867632659\n",
      "0.49:0.10500043599957455:0.13569648296870673:0.11839112803268294\n"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using average as the measure, wordnet similarity\n",
    "#BUG IN COUNTING TP,NP, etc.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_sumval = dict()\n",
    "    d_lenval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 5:\n",
    "            continue\n",
    "            \n",
    "        id = words[0].strip()\n",
    "        label = words[1].strip()\n",
    "        d_actuallabel[id] = label.lower()\n",
    "\n",
    "        if id not in d_sumval.keys():\n",
    "            d_sumval[id] = '0'\n",
    "            d_lenval[id] = '0'\n",
    "            \n",
    "        if float(d_lenval[id]) > 5:\n",
    "            continue\n",
    "        word1 = words[2].strip()\n",
    "        word2 = words[3].strip()\n",
    "        val = words[4].strip()\n",
    "        if words[4] == 'None' or words[4] == '-1':\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        #if (float(d_minval[id]) > f_val):\n",
    "        d_sumval[id] = str(f_val + float(d_sumval[id]))\n",
    "        d_lenval[id] = str(float(d_lenval[id])+1)\n",
    "   \n",
    "    for id in d_sumval.keys():\n",
    "        if (float(d_lenval[id]) != 0 and (float(d_sumval[id])/float(d_lenval[id]) < threshold)):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_predictedlabel[id] == d_actuallabel[id]:\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_predictedlabel[id] == d_actuallabel[id]:\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_predictedlabel[id] != d_actuallabel[id]:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    #print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "    #print('Precision : ' + str(float(tp)/float(tp+fp)))\n",
    "    #print('Recall : '+ str(float(tp)/float(tp+fn)))\n",
    "    #print('Accuracy : '+ str(float(tp+tn)/float(tp+tn+fp+fn)))\n",
    "    #print('F-score : '+ str(fscore))\n",
    "    return (str(threshold)+':'+str(aprecision)+':'+str(arecall)+':'+str(fscore))\n",
    "\n",
    "\n",
    "for i in np.arange(0.1,0.5,0.01):\n",
    "    print(calc_val('booksnippets.o',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.076923076923077 6.0\n",
      "58.333333333333336 5.0\n",
      "14.285714285714286 20.0\n",
      "54.83870967741935 3.0\n",
      "9.25925925925926 1\n",
      "55.76923076923077 1\n",
      "6.818181818181818 1\n",
      "16.216216216216218 1\n",
      "38.888888888888886 1\n",
      "38.028169014084504 1\n",
      "34.72222222222222 1\n",
      "68.18181818181819 2.0\n",
      "35.13513513513514 1\n",
      "7.142857142857143 1\n",
      "19.51219512195122 1\n",
      "6.172839506172839 1\n",
      "5.0 3.0\n",
      "10.81081081081081 1\n",
      "36.666666666666664 1\n",
      "32.857142857142854 1\n",
      "45.45454545454545 4.0\n",
      "26.08695652173913 1\n",
      "44.827586206896555 1\n",
      "8.823529411764707 1\n",
      "50.0 80.0\n",
      "4.545454545454546 2.0\n",
      "56.0 1\n",
      "62.121212121212125 1\n",
      "38.23529411764706 1\n",
      "90.3225806451613 1\n",
      "75.67567567567568 1\n",
      "61.36363636363637 1\n",
      "65.55555555555556 1\n",
      "91.89189189189189 1\n",
      "16.0 1\n",
      "16.9811320754717 1\n",
      "2.7777777777777777 1\n",
      "93.10344827586206 1\n",
      "91.02564102564102 1\n",
      "95.94594594594595 1\n",
      "1.343784994400896 1\n",
      "53.40909090909091 1\n",
      "91.17647058823529 1\n",
      "23.404255319148938 1\n",
      "36.61971830985915 1\n",
      "25.58139534883721 1\n",
      "20.0 27.0\n",
      "88.37209302325581 1\n",
      "77.77777777777777 5.0\n",
      "52.27272727272727 1\n",
      "80.95238095238095 1\n",
      "39.39393939393939 2.0\n",
      "45.0 2.0\n",
      "55.0 3.0\n",
      "28.571428571428573 10.0\n",
      "94.73684210526316 1\n",
      "76.92307692307692 5.0\n",
      "74.07407407407408 1\n",
      "54.54545454545455 3.0\n",
      "26.923076923076923 2.0\n",
      "92.3076923076923 3.0\n",
      "83.87096774193549 2.0\n",
      "87.5 11.0\n",
      "22.857142857142858 1\n",
      "88.88888888888889 8.0\n",
      "72.0 1\n",
      "51.31086142322097 1\n",
      "21.73913043478261 2.0\n",
      "53.84615384615385 8.0\n",
      "87.09677419354838 1\n",
      "9.803921568627452 1\n",
      "31.57894736842105 3.0\n",
      "67.6470588235294 1\n",
      "20.689655172413794 3.0\n",
      "78.125 1\n",
      "75.78947368421052 1\n",
      "88.15789473684211 1\n",
      "57.142857142857146 16.0\n",
      "34.09090909090909 1\n",
      "43.75 1\n",
      "61.29032258064516 3.0\n",
      "89.47368421052632 2.0\n",
      "35.294117647058826 3.0\n",
      "76.77725118483413 1\n",
      "15.0 3.0\n",
      "55.357142857142854 1\n",
      "25.49019607843137 1\n",
      "67.74193548387096 2.0\n",
      "93.75 4.0\n",
      "82.08955223880596 1\n",
      "41.935483870967744 1\n",
      "7.317073170731708 1\n",
      "62.0 1\n",
      "46.8 1\n",
      "76.47058823529412 2.0\n",
      "11.764705882352942 3.0\n",
      "27.77777777777778 3.0\n",
      "94.4954128440367 1\n",
      "24.324324324324323 1\n",
      "96.96969696969697 1\n",
      "71.42857142857143 18.0\n",
      "18.75 6.0\n",
      "24.137931034482758 1\n",
      "90.625 1\n",
      "85.18518518518519 1\n",
      "13.88888888888889 1\n",
      "52.38095238095238 2.0\n",
      "14.965986394557824 1\n",
      "57.5 1\n",
      "56.60377358490566 1\n",
      "91.42857142857143 1\n",
      "25.757575757575758 1\n",
      "12.0 1\n",
      "58.8235294117647 3.0\n",
      "13.333333333333334 6.0\n",
      "16.666666666666668 13.0\n",
      "75.0 29.0\n",
      "94.28571428571429 1\n",
      "26.041666666666668 1\n",
      "70.37037037037037 3.0\n",
      "27.272727272727273 6.0\n",
      "18.181818181818183 5.0\n",
      "59.45945945945946 2.0\n",
      "33.84615384615385 1\n",
      "71.57894736842105 1\n",
      "10.526315789473685 1\n",
      "15.789473684210526 6.0\n",
      "33.333333333333336 56.0\n",
      "70.3125 1\n",
      "38.297872340425535 1\n",
      "35.483870967741936 1\n",
      "97.05882352941177 1\n",
      "19.047619047619047 2.0\n",
      "86.36363636363636 1\n",
      "97.43589743589743 2.0\n",
      "52.94117647058823 3.0\n",
      "24.0 2.0\n",
      "44.44444444444444 5.0\n",
      "38.46153846153846 8.0\n",
      "26.31578947368421 1\n",
      "96.42857142857143 2.0\n",
      "36.36363636363637 9.0\n",
      "47.61904761904762 2.0\n",
      "56.52173913043478 2.0\n",
      "6.315789473684211 1\n",
      "58.064516129032256 2.0\n",
      "78.57142857142857 2.0\n",
      "85.29411764705883 1\n",
      "82.02247191011236 1\n",
      "32.142857142857146 1\n",
      "62.06896551724138 2.0\n",
      "8.571428571428571 1\n",
      "100.0 155.0\n",
      "42.857142857142854 16.0\n",
      "79.59183673469387 1\n",
      "7.6923076923076925 3.0\n",
      "42.30769230769231 3.0\n",
      "73.52941176470588 1\n",
      "35.714285714285715 3.0\n",
      "64.70588235294117 2.0\n",
      "86.95652173913044 4.0\n",
      "31.52173913043478 1\n",
      "5.882352941176471 1\n",
      "3.7735849056603774 1\n",
      "95.45454545454545 1\n",
      "30.76923076923077 5.0\n",
      "6.666666666666667 3.0\n",
      "6.25 1\n",
      "39.53488372093023 1\n",
      "96.55172413793103 1\n",
      "19.444444444444443 2.0\n",
      "96.15384615384616 1\n",
      "73.17073170731707 1\n",
      "65.0 4.0\n",
      "41.666666666666664 5.0\n",
      "91.66666666666667 3.0\n",
      "37.93103448275862 1\n",
      "63.1578947368421 1\n",
      "6.896551724137931 1\n",
      "52.01793721973094 1\n",
      "56.25 2.0\n",
      "2.5641025641025643 2.0\n",
      "10.0 6.0\n",
      "6.122448979591836 1\n",
      "72.41379310344827 1\n",
      "80.0 21.0\n",
      "22.22222222222222 4.0\n",
      "82.85714285714286 1\n",
      "82.6086956521739 1\n",
      "73.91304347826087 1\n",
      "45.714285714285715 1\n",
      "79.5774647887324 1\n",
      "21.875 1\n",
      "15.384615384615385 4.0\n",
      "25.714285714285715 1\n",
      "57.89473684210526 1\n",
      "88.0 1\n",
      "20.361990950226243 1\n",
      "23.943661971830984 1\n",
      "12.5 14.0\n",
      "94.11764705882354 1\n",
      "6.4935064935064934 1\n",
      "37.83783783783784 1\n",
      "29.41176470588235 4.0\n",
      "90.0 5.0\n",
      "60.0 17.0\n",
      "47.05882352941177 1\n",
      "37.5 12.0\n",
      "22.580645161290324 2.0\n",
      "86.44067796610169 1\n",
      "26.470588235294116 2.0\n",
      "86.04651162790698 1\n",
      "94.44444444444444 2.0\n",
      "78.35820895522389 1\n",
      "19.672131147540984 1\n",
      "22.727272727272727 2.0\n",
      "70.27027027027027 1\n",
      "20.833333333333332 2.0\n",
      "40.74074074074074 2.0\n",
      "65.51724137931035 2.0\n",
      "13.793103448275861 2.0\n",
      "46.666666666666664 3.0\n",
      "62.5 7.0\n",
      "9.090909090909092 7.0\n",
      "59.25925925925926 2.0\n",
      "68.75 2.0\n",
      "20.73170731707317 1\n",
      "14.814814814814815 1\n",
      "61.44578313253012 1\n",
      "73.33333333333333 8.0\n",
      "53.225806451612904 1\n",
      "3.5714285714285716 1\n",
      "81.25 9.0\n",
      "45.39007092198582 1\n",
      "93.33333333333333 7.0\n",
      "31.25 2.0\n",
      "12.903225806451612 1\n",
      "80.76923076923077 3.0\n",
      "40.21739130434783 1\n",
      "88.57142857142857 1\n",
      "3.225806451612903 1\n",
      "41.1764705882353 1\n",
      "40.4040404040404 1\n",
      "9.523809523809524 1\n",
      "66.66666666666667 59.0\n",
      "69.23076923076923 4.0\n",
      "34.375 1\n",
      "92.0 3.0\n",
      "39.24050632911393 1\n",
      "6.787330316742081 1\n",
      "61.111111111111114 1\n",
      "78.26086956521739 1\n",
      "19.607843137254903 1\n",
      "63.333333333333336 2.0\n",
      "64.28571428571429 1\n",
      "19.402985074626866 1\n",
      "44.067796610169495 1\n",
      "81.48148148148148 1\n",
      "24.242424242424242 1\n",
      "52.63157894736842 1\n",
      "32.758620689655174 1\n",
      "61.904761904761905 1\n",
      "81.33333333333333 1\n",
      "90.47619047619048 1\n",
      "15.646258503401361 1\n",
      "40.0 28.0\n",
      "26.666666666666668 2.0\n",
      "83.33333333333333 15.0\n",
      "80.64516129032258 1\n",
      "72.58064516129032 1\n",
      "48.148148148148145 2.0\n",
      "11.538461538461538 1\n",
      "19.23076923076923 1\n",
      "51.282051282051285 1\n",
      "25.641025641025642 1\n",
      "72.72727272727273 6.0\n",
      "23.80952380952381 1\n",
      "57.57575757575758 1\n",
      "62.857142857142854 1\n",
      "77.41935483870968 1\n",
      "16.129032258064516 1\n",
      "51.724137931034484 1\n",
      "78.94736842105263 1\n",
      "11.11111111111111 2.0\n",
      "28.0 1\n",
      "96.0 1\n",
      "60.28708133971292 1\n",
      "46.391752577319586 1\n",
      "70.0 6.0\n",
      "61.702127659574465 1\n",
      "63.63636363636363 9.0\n",
      "81.81818181818181 6.0\n",
      "60.714285714285715 1\n",
      "39.130434782608695 1\n",
      "88.23529411764706 2.0\n",
      "53.333333333333336 2.0\n",
      "17.24137931034483 1\n",
      "92.85714285714286 7.0\n",
      "28.78787878787879 1\n",
      "17.647058823529413 1\n",
      "29.166666666666668 2.0\n",
      "82.3529411764706 2.0\n",
      "90.56603773584905 1\n",
      "55.55555555555556 5.0\n",
      "91.30434782608695 1\n",
      "72.09302325581395 1\n",
      "81.13207547169812 1\n",
      "25.925925925925927 1\n",
      "96.29629629629629 1\n",
      "29.23076923076923 1\n",
      "44.642857142857146 1\n",
      "62.96296296296296 1\n",
      "85.71428571428571 12.0\n",
      "84.7457627118644 1\n",
      "36.95652173913044 1\n",
      "52.0 1\n",
      "18.51851851851852 1\n",
      "3.8461538461538463 3.0\n",
      "68.0 1\n",
      "90.9090909090909 3.0\n",
      "11.428571428571429 2.0\n",
      "30.0 7.0\n",
      "30.097087378640776 1\n",
      "23.529411764705884 2.0\n",
      "8.695652173913043 1\n",
      "13.043478260869565 1\n",
      "65.21739130434783 1\n",
      "87.43455497382199 1\n",
      "48.38709677419355 1\n",
      "51.61290322580645 2.0\n",
      "95.65217391304348 1\n",
      "69.6969696969697 2.0\n",
      "61.53846153846154 3.0\n",
      "73.6842105263158 1\n",
      "44.54545454545455 1\n",
      "71.10091743119266 1\n",
      "54.166666666666664 1\n",
      "14.705882352941176 1\n",
      "53.57142857142857 1\n",
      "46.15384615384615 4.0\n",
      "4.761904761904762 1\n",
      "25.0 31.0\n",
      "34.146341463414636 1\n",
      "27.5 1\n",
      "93.20388349514563 1\n",
      "42.05607476635514 1\n",
      "97.67441860465117 1\n",
      "79.16666666666667 1\n",
      "84.61538461538461 6.0\n",
      "26.174496644295303 1\n",
      "8.333333333333334 8.0\n",
      "97.91666666666667 1\n",
      "54.23728813559322 1\n",
      "86.66666666666667 4.0\n",
      "36.8421052631579 1\n",
      "7.407407407407407 2.0\n",
      "30.952380952380953 1\n",
      "28.72340425531915 1\n",
      "45.945945945945944 1\n",
      "43.47826086956522 1\n",
      "5.555555555555555 1\n",
      "39.02439024390244 1\n",
      "23.25581395348837 1\n",
      "21.05263157894737 1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using minimum as the measure - also reporting position\n",
    "#Bug in count of TP, NP\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_posval = dict()\n",
    "    d_lenval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    ip = 0\n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 5:\n",
    "            continue\n",
    "            \n",
    "        id = words[0].strip()\n",
    "        label = words[1].strip()\n",
    "        d_actuallabel[id] = label.lower()\n",
    "\n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "            ip = 0\n",
    "            d_lenval[id] = '0'\n",
    "            \n",
    "        d_lenval[id] = str(float(d_lenval[id])+1)\n",
    "        word1 = words[2].strip()\n",
    "        word2 = words[3].strip()\n",
    "        val = words[4].strip()\n",
    "        if words[4] == 'None' or words[4] == '-1':\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "            d_posval[id] = d_lenval[id]\n",
    "        \n",
    "   \n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_predictedlabel[id] == d_actuallabel[id]:\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_predictedlabel[id] == d_actuallabel[id]:\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_predictedlabel[id] != d_actuallabel[id]:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    \n",
    "    summary = dict()\n",
    "    for id in d_posval.keys():\n",
    "        if str(100*float(d_posval[id])/float(d_lenval[id])) not in summary.keys():\n",
    "            summary[str(100*float(d_posval[id])/float(d_lenval[id]))] = str(1)\n",
    "        else:\n",
    "            summary[str(100*float(d_posval[id])/float(d_lenval[id]))] = str(float(summary[str(100*float(d_posval[id])/float(d_lenval[id]))])+1)\n",
    "    #print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "    #print('Precision : ' + str(float(tp)/float(tp+fp)))\n",
    "    #print('Recall : '+ str(float(tp)/float(tp+fn)))\n",
    "    #print('Accuracy : '+ str(float(tp+tn)/float(tp+tn+fp+fn)))\n",
    "    #print('F-score : '+ str(fscore))\n",
    "    \n",
    "    for id in summary.keys():\n",
    "        print (id+' '+summary[id])\n",
    "    #return (str(threshold)+':'+str(aprecision)+':'+str(arecall)+':'+str(fscore))\n",
    "\n",
    "\n",
    "print(calc_val('iaccorpus.o',0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.1:0.4930083153933176:0.5080691881155341:0.5004254588843134\n",
      "0.11:0.4920529237861577:0.5122072333029877:0.5019278420079378\n",
      "0.12:0.4951962729455027:0.5199867582554001:0.5072888272462276\n",
      "0.13:0.4964985324926388:0.5247868906728461:0.5102509351065585\n",
      "0.14:0.4959057521178353:0.5277662832078126:0.5113402077714203\n",
      "0.15:0.4951467953429072:0.5302491103202847:0.5120971251366706\n",
      "0.16:0.49380748618737846:0.5320698502027642:0.5122251284529822\n",
      "0.17:0.49505947785298643:0.5352147645452289:0.5143545882661369\n",
      "0.18:0.49487715324064807:0.53778035256145:0.5154375162123994\n",
      "0.19:0.49496554015588634:0.5400148969626748:0.516509792033367\n",
      "0.2:0.4927378360219802:0.5406769841926674:0.5155954839560936\n",
      "0.21:0.5051831211028865:0.5466357692626004:0.5250926115742236\n",
      "0.22:0.5064663026045898:0.5480427046263345:0.5264348817851978\n",
      "0.23:0.5019593692327629:0.5475461392038401:0.5237626910029632\n",
      "0.24:0.5005984209447485:0.5478771828188365:0.5231718346259389\n",
      "0.25:0.5233745322890058:0.5536704460812713:0.538096396305574\n",
      "0.26:0.5202729377859758:0.5532566415625259:0.5362580850916535\n",
      "0.27:0.5284118226092178:0.5548290987337582:0.5412983382035816\n",
      "0.28:0.5291971457994689:0.5549946205412564:0.5417889680453293\n",
      "0.29:0.550762165688463:0.5578912521724737:0.5543037874866349\n",
      "0.3:0.5471236049000053:0.5573119258462302:0.5521707721893541\n",
      "0.31:0.5452369075172345:0.556898121327485:0.5510058232937912\n",
      "0.32:0.5410705814620651:0.5564015559049905:0.5486289867953563\n",
      "0.33:0.5439690174341685:0.5566498386162376:0.5502363767482021\n",
      "0.34:0.5420668984555206:0.5564015559049905:0.5491406958624567\n",
      "0.35:0.5371374146764303:0.5559049904824961:0.5463600826174314\n",
      "0.36:0.5323696345741701:0.5554911859637507:0.5436846958684746\n",
      "0.37:0.5292048462371921:0.5552429032525035:0.5419112822693452\n",
      "0.38:0.5301976842168901:0.5553256641562526:0.5424708396426717\n",
      "0.39:0.5278290562008244:0.5551601423487545:0.5411497259043875\n",
      "0.4:0.5265466296816161:0.5550773814450054:0.5404357177831716\n",
      "0.41:0.5240260685810838:0.5549118596375072:0.5390268941513155\n",
      "0.42:0.5262659725942748:0.5550773814450054:0.5402878501450109\n",
      "0.43:0.5286900754680095:0.5552429032525036:0.5416412604590263\n",
      "0.44:0.52195306121793:0.5548290987337582:0.5378891985912237\n",
      "0.45:0.5144466132467065:0.5544152942150129:0.5336836656821444\n",
      "0.46:0.512937732215589:0.5543325333112639:0.5328323700457259\n",
      "0.47:0.5124700643752967:0.5543325333112638:0.532579934933399\n",
      "0.48:0.5135704868956534:0.5544152942150129:0.5332118416338669\n",
      "0.49:0.5133373906544298:0.5544152942150128:0.5330861809184622\n"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using word2vec similarity and minimum as the measure\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 5:\n",
    "            continue\n",
    "            \n",
    "        id = words[0].strip()\n",
    "#        if id not in acceptable:\n",
    "#            continue\n",
    "        label = words[1].strip()\n",
    "        d_actuallabel[id] = label.lower()\n",
    "\n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "        word1 = words[2].strip()\n",
    "        word2 = words[3].strip()\n",
    "        #val = words[4].strip()\n",
    "        #if words[4] == 'None' or words[4] == '-1':\n",
    "        #    continue\n",
    "        \n",
    "        if word1.strip().lower() in mo.vocab and word2.strip().lower() in mo.vocab:\n",
    "            val = str(mo.similarity(word1,word2))\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if val == -1:\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "\n",
    "   \n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    #print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "    #print('Precision : ' + str(float(tp)/float(tp+fp)))\n",
    "    #print('Recall : '+ str(float(tp)/float(tp+fn)))\n",
    "    #print('Accuracy : '+ str(float(tp+tn)/float(tp+tn+fp+fn)))\n",
    "    #print('F-score : '+ str(fscore))\n",
    "    return (str(threshold)+':'+str(aprecision)+':'+str(arecall)+':'+str(fscore))\n",
    "\n",
    "acceptable = []\n",
    "#fk = open('/Users/adityaj/work/Sem8/Datasets/BookSnippets/all.stats','r')\n",
    "#for line in fk:\n",
    " #   words = line.split('\\t')\n",
    "  #  fkid = int(words[0])\n",
    "   # fklen = int(words[1])\n",
    "    #if fklen < 120:\n",
    "     #   acceptable.append(fkid)\n",
    "print(len(acceptable))\n",
    "for i in np.arange(0.1,0.5,0.01):\n",
    "    print(calc_val('tweetsall.o',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec as wv\n",
    "#mo = wv.load_word2vec_format('/Users/adityaj/DeepLearning/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "\n",
    "def get_similarity(w1,w2,mo):\n",
    "    if w1 == w2:\n",
    "        return (1)\n",
    "    \n",
    "    if len(wn.synsets(w1)) == 0:\n",
    "        return (1)\n",
    "    \n",
    "    if len(wn.synsets(w1)) == 0:\n",
    "        return (1)\n",
    "    s_syn1 = wn.synsets(w1)[0]\n",
    "    s_syn2 = wn.synsets(w2)[0]\n",
    "    \n",
    "    syn1 = wn.synsets(w1)[0]\n",
    "    \n",
    "    if syn1.lemmas.size() == 0:\n",
    "        print('yy')\n",
    "        return (1)\n",
    "    \n",
    "    l = (syn1.lemmas[0])\n",
    "    \n",
    "    if len(l.antonyms()) != 0:\n",
    "        l2 = (l.antonyms()[0])\n",
    "        t2 = l2.name\n",
    "        \n",
    "        if t2.strip() == w2.strip():\n",
    "            return(0)\n",
    "    return (1)\n",
    "\n",
    "\n",
    "mo = ()\n",
    "\n",
    "print(get_similarity('joshi','subtle',mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-91686f8c1ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalc_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweets.o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-91686f8c1ab4>\u001b[0m in \u001b[0;36mcalc_val\u001b[0;34m(file, v_threshold)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mword2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer' is not defined"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using word2vec similarity, and top 50% words only and minimum as the measure\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from gensim.models import Word2Vec as wv\n",
    "#mo = wv.load_word2vec_format('/Users/adityaj/DeepLearning/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "         \n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 5:\n",
    "            continue\n",
    "            \n",
    "        id = words[0].strip()\n",
    "        label = words[1].strip()\n",
    "        d_actuallabel[id] = label.lower()\n",
    "\n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "        \n",
    "        word1 = words[2].strip()\n",
    "        word2 = words[3].strip()\n",
    "        \n",
    "        if str(id) in answer.keys() and word1.strip() in answer[str(id)]:\n",
    "            val = words[4].strip()\n",
    "        elif str(id) not in answer.keys():\n",
    "            val = words[4].strip()\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if words[4] == 'None' or words[4] == '-1':\n",
    "            continue\n",
    "        \n",
    "        if str(id) in answer.keys() and word1.strip() in answer[str(id)] and word1.strip().lower() in mo.vocab and word2.strip().lower() in mo.vocab:\n",
    "            val = str(mo.similarity(word1,word2))\n",
    "        elif str(id) not in answer.keys() and word1.strip().lower() in mo.vocab and word2.strip().lower() in mo.vocab:\n",
    "            val = str(mo.similarity(word1,word2))\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if val == -1:\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "\n",
    "   \n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    if float(tp+fp)==0 or float(tp+fn) == 0 or float (tn+fn) == 0 or float(tn+fp) == 0:\n",
    "        return ('none')\n",
    "        \n",
    "\n",
    "    #print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    return (str(threshold)+':'+str(aprecision)+':'+str(arecall)+':'+str(fscore))\n",
    "\n",
    "\n",
    "for i in np.arange(0.1,0.5,0.01):\n",
    "    print(calc_val('tweets.o',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-aadd903e6a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0md_allwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'booksnippets.o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mget_top_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_allwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-aadd903e6a9d>\u001b[0m in \u001b[0;36mget_top_words\u001b[0;34m(d_allwords)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mssum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[1;32mif\u001b[0m \u001b[0mlist1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlist1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlist1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0mavg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mssum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "# Get top 50% words which are most dissimilar\n",
    "\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "temp_list = list()\n",
    "\n",
    "def get_all_words(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_allwords = dict()\n",
    "    d_topwords = dict()\n",
    "    old_id = 1\n",
    "    temp_list = list()\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 5:\n",
    "            continue\n",
    "        \n",
    "        id = words[0].strip()\n",
    "       \n",
    "        if str(old_id) != id:\n",
    "            d_allwords[id] = temp_list\n",
    "            \n",
    "            #print(temp_list)\n",
    "            temp_list = list()\n",
    "            old_id = str(id)\n",
    "\n",
    "        word1 = words[2].strip()\n",
    "        temp_list.append(word1)\n",
    "\n",
    "    return d_allwords\n",
    "        \n",
    "answer = dict()\n",
    "\n",
    "def get_top_words(d_allwords):\n",
    "    for k in d_allwords.keys():\n",
    "        list1 = d_allwords[k]\n",
    "        wordsim = dict()\n",
    "        for i in range(0,len(list1)):\n",
    "            avg = 0\n",
    "            ssum = 0\n",
    "            for j in range(0,len(list1)):\n",
    "                if list1[i] != list1[j] and list1[i] in mo.vocab and list1[j] in mo.vocab:\n",
    "                    avg += mo.similarity(list1[i],list1[j])\n",
    "                    ssum += 1\n",
    "            \n",
    "            if ssum != 0:\n",
    "                avg = avg/ssum\n",
    "                wordsim[list1[i]] = avg\n",
    "                #print(str(i)+' '+str(wordsim[list1[i]])+' '+ str(list1[i]))\n",
    "        \n",
    "        x = wordsim\n",
    "        #x = {1: 2, 3: 4, 4: 3, 2: 1, 12: 0}\n",
    "        sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "        l2 = list()\n",
    "        for i in range(0, int(len(sorted_x)/2)):\n",
    "            l2.append(sorted_x[i][0])\n",
    "        answer[k] = l2\n",
    "        #print(str(k)+' '+str(l2))\n",
    "        #print (sorted_x)\n",
    "        \n",
    "        \n",
    "d_allwords = get_all_words('booksnippets.o',1)\n",
    "get_top_words(d_allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/adityaj/DeepLearning/GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aa3ee0b2fef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/adityaj/DeepLearning/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[1;31m# local files -- both read & write supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[1;31m# compression, if any, is determined by the filename extension (.gz, .bz2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3u\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[0;34m(fname, mode)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \"\"\"\n\u001b[0;32m--> 642\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/adityaj/DeepLearning/GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors as wv\n",
    "mo = wv.load_word2vec_format('/Users/adityaj/DeepLearning/GoogleNews-vectors-negative300.bin', binary = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "x = {1: 2, 3: 4, 4: 3, 2: 1, 12: 0}\n",
    "sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "print (sorted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ca561e68e363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'answer' is not defined"
     ]
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open('/Users/adityaj/work/Sem8/Datasets/BookSnippets/all','r')\n",
    "f2 = open('/Users/adityaj/work/Sem8/Datasets/BookSnippets/all.stats','w')\n",
    "min = 1000000\n",
    "max = 0\n",
    "id = 1\n",
    "for line in f:\n",
    "    words = line.strip().split('\\t')\n",
    "    text=words[0]\n",
    "    f2.write(str(id)+'\\t'+str(len(text))+'\\n')\n",
    "    id += 1\n",
    "    if (min>len(text)):\n",
    "        min = len(text)\n",
    "    if (max<=len(text)):\n",
    "        max = len(text)\n",
    "#print(str(min)+' '+str(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors as wv\n",
    "mo = wv.load_word2vec_format('C:/Users/Samarth/Documents/Projects/GoogleNews-vectors-negative300.bin', binary = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4346c959323a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'answer' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1:0.37593984962406013:0.06784260515603799:0.46867749419953597:0.8795355587808418\n",
      "0.11:0.4318936877076412:0.17639077340569878:0.46044444444444443:0.7518142235123367\n",
      "0.12:0.4616822429906542:0.33514246947082765:0.4500561167227834:0.5820029027576198\n",
      "0.13:0.47166921898928027:0.417910447761194:0.445019404915912:0.49927431059506533\n",
      "0.14:0.4783180026281209:0.49389416553595655:0.43909774436090226:0.42380261248185774\n",
      "0.15:0.4744958481613286:0.5427408412483039:0.4219554030874786:0.3570391872278665\n",
      "0.16:0.47811816192560175:0.592944369063772:0.4140625:0.3076923076923077\n",
      "0.17:0.48020833333333335:0.6255088195386703:0.40772532188841204:0.2757619738751814\n",
      "0.18:0.48020833333333335:0.6255088195386703:0.40772532188841204:0.2757619738751814\n",
      "0.19:0.48370672097759676:0.6445047489823609:0.4099099099099099:0.2641509433962264\n",
      "0.2:0.48484848484848486:0.6512890094979648:0.4105504587155963:0.2597968069666183\n",
      "0.21:0.4900398406374502:0.6675712347354138:0.4194312796208531:0.25689404934687954\n",
      "0.22:0.4886922320550639:0.6743554952510177:0.4132029339853301:0.24528301886792453\n",
      "0.23:0.4899328859060403:0.6933514246947082:0.40992167101827676:0.22786647314949202\n",
      "0.24:0.49065420560747663:0.7123473541383989:0.4044943820224719:0.20899854862119013\n",
      "0.25:0.49065420560747663:0.7123473541383989:0.4044943820224719:0.20899854862119013\n",
      "0.26:0.49417040358744396:0.7476255088195387:0.40192926045016075:0.18142235123367198\n",
      "0.27:0.4960212201591512:0.7611940298507462:0.4033898305084746:0.17271407837445574\n",
      "0.28:0.495583038869258:0.7611940298507462:0.4013605442176871:0.17126269956458637\n",
      "0.29:0.49828473413379076:0.7883310719131614:0.4:0.1509433962264151\n",
      "0.3:0.49828473413379076:0.7883310719131614:0.4:0.1509433962264151\n",
      "0.31:0.498330550918197:0.8100407055630936:0.38596491228070173:0.12772133526850507\n",
      "0.32:0.4970857618651124:0.8100407055630936:0.37777777777777777:0.12336719883889695\n",
      "0.33:0.4970857618651124:0.8100407055630936:0.37777777777777777:0.12336719883889695\n",
      "0.34:0.49917627677100496:0.8222523744911805:0.38207547169811323:0.11756168359941944\n",
      "0.35:0.49917763157894735:0.8236092265943012:0.38095238095238093:0.11611030478955008\n",
      "0.36:0.5004095004095004:0.8290366350067843:0.3853658536585366:0.11465892597968069\n",
      "0.37:0.500406834825061:0.8344640434192673:0.38071065989847713:0.10885341074020319\n",
      "0.38:0.4991883116883117:0.8344640434192673:0.3711340206185567:0.10449927431059507\n",
      "0.39:0.49878345498783455:0.8344640434192673:0.36787564766839376:0.10304789550072568\n",
      "0.4:0.49878345498783455:0.8344640434192673:0.36787564766839376:0.10304789550072568\n",
      "0.41:0.499597747385358:0.8426051560379919:0.366120218579235:0.09724238026124818\n",
      "0.42:0.499597747385358:0.8426051560379919:0.366120218579235:0.09724238026124818\n",
      "0.43:0.5012009607686149:0.8493894165535957:0.3728813559322034:0.09579100145137881\n",
      "0.44:0.5012009607686149:0.8493894165535957:0.3728813559322034:0.09579100145137881\n",
      "0.45:0.50199203187251:0.8548168249660787:0.3742690058479532:0.09288824383164006\n",
      "0.46:0.50199203187251:0.8548168249660787:0.3742690058479532:0.09288824383164006\n",
      "0.47:0.5015898251192369:0.8561736770691994:0.36904761904761907:0.0899854862119013\n",
      "0.48:0.5023771790808241:0.8602442333785617:0.3719512195121951:0.08853410740203194\n",
      "0.49:0.5023771790808241:0.8602442333785617:0.3719512195121951:0.08853410740203194\n"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using minimum as the measure\n",
    "#Print class-wise Precision and Recall\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 5:\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        id = words[0].strip()\n",
    "        label = words[1].strip()\n",
    "        d_actuallabel[id] = label.lower()\n",
    "        \n",
    "#        if int(id) not in acceptable:\n",
    "#            continue\n",
    "            \n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "        word1 = words[2].strip()\n",
    "        word2 = words[3].strip()\n",
    "        val = words[4].strip()\n",
    "        if words[4] == 'None' or words[4] == '-1':\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "\n",
    "   \n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    #print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "    #print('Precision : ' + str(float(tp)/float(tp+fp)))\n",
    "    #print('Recall : '+ str(float(tp)/float(tp+fn)))\n",
    "    #print('Accuracy : '+ str(float(tp+tn)/float(tp+tn+fp+fn)))\n",
    "    #print('F-score : '+ str(fscore))\n",
    "    return (str(threshold)+':'+str(pprecision)+':'+str(precall)+':'+str(nprecision)+':'+str(nrecall))\n",
    "\n",
    "acceptable = []\n",
    "#fk = open('/Users/adityaj/work/Sem8/Datasets/BookSnippets/all.stats','r')\n",
    "#for line in fk:\n",
    "#    words = line.split('\\t')\n",
    "#    fkid = int(words[0])\n",
    "#    fklen = int(words[1])\n",
    "#    if fklen <= 120:\n",
    "#        acceptable.append(fkid)\n",
    "#print(len(acceptable))\n",
    "for i in np.arange(0.1,0.5,0.01):\n",
    "    print(calc_val('iaccorpus.o',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.1:0.49635701275045535:0.7394843962008141:0.4146341463414634:0.19738751814223512\n",
      "0.11:0.4978127734033246:0.7720488466757124:0.40636042402826855:0.16690856313497823\n",
      "0.12:0.4965753424657534:0.7869742198100407:0.39147286821705424:0.14658925979680695\n",
      "0.13:0.49876135425268375:0.819538670284939:0.3813953488372093:0.11901306240928883\n",
      "0.14:0.5012126111560227:0.841248303934871:0.38095238095238093:0.10449927431059507\n",
      "0.15:0.5024038461538461:0.8507462686567164:0.38202247191011235:0.09869375907111756\n",
      "0.16:0.5067087608524072:0.8710990502035278:0.4025157232704403:0.09288824383164006\n",
      "0.17:0.5058639562157936:0.8778833107191316:0.3877551020408163:0.08272859216255443\n",
      "0.18:0.5077519379844961:0.8887381275440976:0.39705882352941174:0.0783744557329463\n",
      "0.19:0.511128165771297:0.903663500678426:0.42276422764227645:0.07547169811320754\n",
      "0.2:0.5121765601217656:0.9131614654002713:0.42857142857142855:0.06966618287373004\n",
      "0.21:0.514004542013626:0.921302578018996:0.44761904761904764:0.06821480406386067\n",
      "0.22:0.5146948003014318:0.926729986431479:0.45454545454545453:0.06531204644412192\n",
      "0.23:0.5175766641735228:0.9389416553595658:0.4943820224719101:0.06386066763425254\n",
      "0.24:0.5156482861400894:0.9389416553595658:0.4642857142857143:0.05660377358490566\n",
      "0.25:0.5156017830609212:0.9416553595658074:0.4625:0.05370101596516691\n",
      "0.26:0.5151964418087472:0.9430122116689281:0.45454545454545453:0.05079825834542816\n",
      "0.27:0.5155096011816839:0.9470827679782904:0.4583333333333333:0.047895500725689405\n",
      "0.28:0.5158204562178073:0.9511533242876526:0.4626865671641791:0.04499274310595065\n",
      "0.29:0.5172413793103449:0.9565807327001357:0.49206349206349204:0.04499274310595065\n",
      "0.3:0.5160818713450293:0.9579375848032564:0.46551724137931033:0.03918722786647315\n",
      "0.31:0.5167394468704513:0.9633649932157394:0.4807692307692308:0.036284470246734396\n",
      "0.32:0.5167151162790697:0.9647218453188603:0.48:0.03483309143686502\n",
      "0.33:0.516642547033285:0.9687924016282226:0.4772727272727273:0.030478955007256895\n",
      "0.34:0.5162689804772235:0.9687924016282226:0.46511627906976744:0.02902757619738752\n",
      "0.35:0.5169675090252708:0.9715061058344641:0.4878048780487805:0.02902757619738752\n",
      "0.36:0.5173160173160173:0.9728629579375848:0.5:0.02902757619738752\n",
      "0.37:0.517664023071377:0.9742198100407056:0.5128205128205128:0.02902757619738752\n",
      "0.38:0.5183585313174947:0.9769335142469471:0.5405405405405406:0.02902757619738752\n",
      "0.39:0.5183585313174947:0.9769335142469471:0.5405405405405406:0.02902757619738752\n",
      "0.4:0.518705035971223:0.9782903663500678:0.5555555555555556:0.02902757619738752\n",
      "0.41:0.518705035971223:0.9782903663500678:0.5555555555555556:0.02902757619738752\n",
      "0.42:0.5190510424155284:0.9796472184531886:0.5714285714285714:0.02902757619738752\n",
      "0.43:0.5193965517241379:0.9810040705563093:0.5882352941176471:0.02902757619738752\n",
      "0.44:0.5193965517241379:0.9810040705563093:0.5882352941176471:0.02902757619738752\n",
      "0.45:0.5193965517241379:0.9810040705563093:0.5882352941176471:0.02902757619738752\n",
      "0.46:0.5197415649676956:0.9823609226594301:0.6060606060606061:0.02902757619738752\n",
      "0.47:0.5197415649676956:0.9823609226594301:0.6060606060606061:0.02902757619738752\n",
      "0.48:0.5197415649676956:0.9823609226594301:0.6060606060606061:0.02902757619738752\n",
      "0.49:0.5193687230989957:0.9823609226594301:0.59375:0.027576197387518143\n"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using word2vec similarity and minimum as the measure\n",
    "#Class-wise precision recall prints\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 5:\n",
    "            continue\n",
    "            \n",
    "        id = words[0].strip()\n",
    "#        if id not in acceptable:\n",
    "#            continue\n",
    "        label = words[1].strip()\n",
    "        d_actuallabel[id] = label.lower()\n",
    "\n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "        word1 = words[2].strip()\n",
    "        word2 = words[3].strip()\n",
    "        #val = words[4].strip()\n",
    "        #if words[4] == 'None' or words[4] == '-1':\n",
    "        #    continue\n",
    "        \n",
    "        if word1.strip().lower() in mo.vocab and word2.strip().lower() in mo.vocab:\n",
    "            val = str(mo.similarity(word1,word2))\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if val == -1:\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "\n",
    "   \n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    #print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "    #print('Precision : ' + str(float(tp)/float(tp+fp)))\n",
    "    #print('Recall : '+ str(float(tp)/float(tp+fn)))\n",
    "    #print('Accuracy : '+ str(float(tp+tn)/float(tp+tn+fp+fn)))\n",
    "    #print('F-score : '+ str(fscore))\n",
    "    return (str(threshold)+':'+str(pprecision)+':'+str(precall)+':'+str(nprecision)+':'+str(nrecall))\n",
    "\n",
    "acceptable = []\n",
    "fk = open('/Users/adityaj/work/Sem8/Datasets/BookSnippets/all.stats','r')\n",
    "#for line in fk:\n",
    " #   words = line.split('\\t')\n",
    "  #  fkid = int(words[0])\n",
    "   # fklen = int(words[1])\n",
    "    #if fklen < 120:\n",
    "     #   acceptable.append(fkid)\n",
    "print(len(acceptable))\n",
    "for i in np.arange(0.1,0.5,0.01):\n",
    "    print(calc_val('iaccorpus.o',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7559039ecb55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalc_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iaccorpus.o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-7559039ecb55>\u001b[0m in \u001b[0;36mcalc_val\u001b[0;34m(file, v_threshold)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mword2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer' is not defined"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using word2vec similarity, and top 50% words only and minimum as the measure\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from gensim.models import Word2Vec as wv\n",
    "#mo = wv.load_word2vec_format('/Users/adityaj/DeepLearning/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "         \n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 5:\n",
    "            continue\n",
    "            \n",
    "        id = words[0].strip()\n",
    "        label = words[1].strip()\n",
    "        d_actuallabel[id] = label.lower()\n",
    "\n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "        \n",
    "        word1 = words[2].strip()\n",
    "        word2 = words[3].strip()\n",
    "        \n",
    "        if str(id) in answer.keys() and word1.strip() in answer[str(id)]:\n",
    "            val = words[4].strip()\n",
    "        elif str(id) not in answer.keys():\n",
    "            val = words[4].strip()\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if words[4] == 'None' or words[4] == '-1':\n",
    "            continue\n",
    "        \n",
    "        if str(id) in answer.keys() and word1.strip() in answer[str(id)] and word1.strip().lower() in mo.vocab and word2.strip().lower() in mo.vocab:\n",
    "            val = str(mo.similarity(word1,word2))\n",
    "        elif str(id) not in answer.keys() and word1.strip().lower() in mo.vocab and word2.strip().lower() in mo.vocab:\n",
    "            val = str(mo.similarity(word1,word2))\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if val == -1:\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "\n",
    "   \n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    if float(tp+fp)==0 or float(tp+fn) == 0 or float (tn+fn) == 0 or float(tn+fp) == 0:\n",
    "        return ('none')\n",
    "        \n",
    "\n",
    "    #print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    return (str(threshold)+':'+str(pprecision)+':'+str(precall)+':'+str(nprecision)+':'+str(nrecall))\n",
    "\n",
    "\n",
    "for i in np.arange(0.1,0.5,0.01):\n",
    "    print(calc_val('iaccorpus.o',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2c94c4186f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalc_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweets.o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-2c94c4186f25>\u001b[0m in \u001b[0;36mcalc_val\u001b[0;34m(file, v_threshold)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mword2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer' is not defined"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using wordnet similarity, and top 50% words only and minimum as the measure\n",
    "#Class-wise statistics\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from gensim.models import Word2Vec as wv\n",
    "#mo = wv.load_word2vec_format('/Users/adityaj/DeepLearning/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "         \n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 5:\n",
    "            continue\n",
    "            \n",
    "        id = words[0].strip()\n",
    "        label = words[1].strip()\n",
    "        d_actuallabel[id] = label.lower()\n",
    "\n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "        \n",
    "        word1 = words[2].strip()\n",
    "        word2 = words[3].strip()\n",
    "        \n",
    "        if str(id) in answer.keys() and word1.strip() in answer[str(id)]:\n",
    "            val = words[4].strip()\n",
    "        elif str(id) not in answer.keys():\n",
    "            val = words[4].strip()\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if words[4] == 'None' or words[4] == '-1':\n",
    "            continue\n",
    "        \n",
    "#        if str(id) in answer.keys() and word1.strip() in answer[str(id)] and word1.strip().lower() in mo.vocab and word2.strip().lower() in mo.vocab:\n",
    "#            val = str(mo.similarity(word1,word2))\n",
    "#        elif str(id) not in answer.keys() and word1.strip().lower() in mo.vocab and word2.strip().lower() in mo.vocab:\n",
    "#            val = str(mo.similarity(word1,word2))\n",
    "\n",
    "#        else:\n",
    "#            continue\n",
    "            \n",
    "        if val == -1:\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "\n",
    "   \n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    if float(tp+fp)==0 or float(tp+fn) == 0 or float (tn+fn) == 0 or float(tn+fp) == 0:\n",
    "        return ('none')\n",
    "        \n",
    "\n",
    "    #print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    return (str(threshold)+':'+str(pprecision)+':'+str(precall)+':'+str(nprecision)+':'+str(nrecall))\n",
    "\n",
    "\n",
    "for i in np.arange(0.1,0.5,0.01):\n",
    "    print(calc_val('tweets.o',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.1:0.5907809152443424:0.7681992337164751:0.6679088678845843\n",
      "-19.09:0.5907809152443424:0.7681992337164751:0.6679088678845843\n",
      "-19.08:0.5907809152443424:0.7681992337164751:0.6679088678845843\n",
      "-19.07:0.5907809152443424:0.7681992337164751:0.6679088678845843\n",
      "-19.06:0.5907809152443424:0.7681992337164751:0.6679088678845843\n",
      "-19.05:0.5907809152443424:0.7681992337164751:0.6679088678845843\n",
      "-19.04:0.5907809152443424:0.7681992337164751:0.6679088678845843\n",
      "-19.03:0.5907809152443424:0.7681992337164751:0.6679088678845843\n",
      "-19.02:0.5907809152443424:0.7681992337164751:0.6679088678845843\n",
      "-19.01:0.5907809152443424:0.7681992337164751:0.6679088678845843\n",
      "-19.0:0.5907809152443424:0.7681992337164751:0.6679088678845843\n"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using minimum as the measure. Metric in this case is what is returned by context2vec \n",
    "#Print class-wise Precision and Recall\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    prev_text = \"\"\n",
    "    prev_label = \"\"\n",
    "    min_val = 0\n",
    "    id = 0\n",
    "    \n",
    "# remember: 0.54238653183 : remember : 0.542387 : 0 \n",
    "# do you remember me telling you we are practicing non-verbal spells, potter?\" \"yes,\" said harry stiffly. \"yes, sir.\" \"there's no need to call me \"sir\" professor.\" the words had escaped him before he knew what he was saying. # sarcasm\n",
    "    for line in f:\n",
    "        words = line.strip().split('#')\n",
    "        if len(words) != 3:\n",
    "            continue\n",
    "        data = words[0].strip()\n",
    "        text = words[1].strip()\n",
    "        label = words[2].strip().lower()\n",
    "\n",
    "        \n",
    "        if prev_text != text:\n",
    "            d_actuallabel[id] = prev_label.strip()\n",
    "            d_minval[id] = min_val\n",
    "            min_val = 0\n",
    "            id += 1\n",
    "        prev_label = label\n",
    "        prev_text = text\n",
    "#        if int(id) not in acceptable:\n",
    "#            continue\n",
    "\n",
    "        datas = data.strip().split(':')\n",
    "        if (len(datas) != 5):\n",
    "            continue\n",
    "        if (datas[3].strip()=='') or (datas[1].strip()==''):\n",
    "            continue\n",
    "        value1 = float(datas[1].strip())\n",
    "        value2 = float(datas[3].strip())\n",
    "       \n",
    "        if (value2 - value1)==0:\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = math.log(abs(value2-value1))\n",
    "            \n",
    "        if min_val > similarity:\n",
    "            min_val = similarity  \n",
    "            \n",
    "    overall_min = 0\n",
    "    overall_max = -10000\n",
    "    for id in d_minval.keys():\n",
    "        if float(d_minval[id]) > overall_max:\n",
    "            overall_max = float(d_minval[id])\n",
    "        if float(d_minval[id]) < overall_min:\n",
    "            overall_min = float(d_minval[id])\n",
    "    #print(str(overall_max) +' '+str(overall_min))\n",
    "\n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] != 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "       \n",
    "    #print(str(tp)+' '+str(tn)+ ' '+str(fp)+' '+str(fn))\n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    #return \"\"\n",
    "    return (str(threshold)+':'+str(aprecision)+':'+str(arecall)+':'+str(fscore))\n",
    "\n",
    "\n",
    "\n",
    "for i in np.arange(-19.1,-19,0.01):\n",
    "    print(calc_val('/Users/adityaj/DeepLearning/Datasets/17nov2016/Tweets.thirdapproach15nov',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Rule-based Approach using minimum as the measure. Metric in this case is what is returned by context2vec \n",
    "#Print class-wise Precision and Recall\n",
    "# 50% words\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_top_words(sentence,word):\n",
    "    d_allwords = []\n",
    "    \n",
    "    words = sentence.strip().lower().split(' ')\n",
    "    for word in words:\n",
    "        d_allwords.append(word[i])\n",
    "    d_sum\n",
    "    for i in range(0,len(d_allwords)):\n",
    "        sum = 0\n",
    "        count = 0\n",
    "        for j in range(0,len(d_allwords)):\n",
    "            if i != j and d_allwords and d_allwords[i] in mo.vocab and d_allwords[j] in mo.vocab:\n",
    "                sum += mo.similarity(d_allwords[i],d_allwords[j])\n",
    "                count += 0\n",
    "        \n",
    "for i in range(0,len(list1)):\n",
    "            avg = 0\n",
    "            ssum = 0\n",
    "            for j in range(0,len(list1)):\n",
    "                if list1[i] != list1[j] and list1[i] in mo.vocab and list1[j] in mo.vocab:\n",
    "                    avg += mo.similarity(list1[i],list1[j])\n",
    "                    ssum += 1\n",
    "            \n",
    "            if ssum != 0:\n",
    "                avg = avg/ssum\n",
    "                wordsim[list1[i]] = avg\n",
    "                #print(str(i)+' '+str(wordsim[list1[i]])+' '+ str(list1[i]))\n",
    "        \n",
    "        x = wordsim\n",
    "        #x = {1: 2, 3: 4, 4: 3, 2: 1, 12: 0}\n",
    "        sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "        l2 = list()\n",
    "        for i in range(0, int(len(sorted_x)/2)):\n",
    "            l2.append(sorted_x[i][0])\n",
    "        answer[k] = l2\n",
    "        #print(str(k)+' '+str(l2))\n",
    "        #print (sorted_x)\n",
    "        \n",
    "        \n",
    "d_allwords = get_all_words('booksnippets.o',1)\n",
    "get_top_words(d_allwords)\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    prev_text = \"\"\n",
    "    prev_label = \"\"\n",
    "    min_val = 0\n",
    "    id = 0\n",
    "\n",
    "# remember: 0.54238653183 : remember : 0.542387 : 0 \n",
    "# do you remember me telling you we are practicing non-verbal spells, potter?\" \"yes,\" said harry stiffly. \"yes, sir.\" \"there's no need to call me \"sir\" professor.\" the words had escaped him before he knew what he was saying. # sarcasm\n",
    "    for line in f:\n",
    "        words = line.strip().split('#')\n",
    "        if len(words) != 3:\n",
    "            continue\n",
    "        data = words[0].strip()\n",
    "        text = words[1].strip()\n",
    "        label = words[2].strip().lower()\n",
    "\n",
    "        \n",
    "        if prev_text != text:\n",
    "            d_actuallabel[id] = prev_label.strip()\n",
    "            d_minval[id] = min_val\n",
    "            min_val = 0\n",
    "            \n",
    "            id += 1\n",
    "        prev_label = label\n",
    "        prev_text = text\n",
    "#        if int(id) not in acceptable:\n",
    "#            continue\n",
    "\n",
    "        datas = data.strip().split(':')\n",
    "        if (len(datas) != 5):\n",
    "            continue\n",
    "        if (datas[3].strip()=='') or (datas[1].strip()==''):\n",
    "            continue\n",
    "        value1 = float(datas[1].strip())\n",
    "        value2 = float(datas[3].strip())\n",
    "        valuerank = float(datas[4].strip())\n",
    "        l_rank.append(valuerank)\n",
    "        if (value2 - value1)==0:\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = math.log(abs(value2-value1))\n",
    "            \n",
    "        if min_val > similarity:\n",
    "            min_val = similarity  \n",
    "            \n",
    "    overall_min = 0\n",
    "    overall_max = -10000\n",
    "    for id in d_minval.keys():\n",
    "        if float(d_minval[id]) > overall_max:\n",
    "            overall_max = float(d_minval[id])\n",
    "        if float(d_minval[id]) < overall_min:\n",
    "            overall_min = float(d_minval[id])\n",
    "    #print(str(overall_max) +' '+str(overall_min))\n",
    "\n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] != 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "       \n",
    "    #print(str(tp)+' '+str(tn)+ ' '+str(fp)+' '+str(fn))\n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    #return \"\"\n",
    "    return (str(threshold)+':'+str(aprecision)+':'+str(arecall)+':'+str(fscore))\n",
    "\n",
    "\n",
    "\n",
    "for i in np.arange(-19.1,-19.04,0.01):\n",
    "    print(calc_val('/Users/adityaj/DeepLearning/Datasets/17nov2016/BookSnippets.thirdapproach15nov',i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Rule-based Approach using word2vec similarity and minimum as the measure\n",
    "#This is word2vec similarity for localized approach\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def calc_val(file,v_threshold):\n",
    "    f = open (file,'r')\n",
    "    d_minval = dict()\n",
    "    d_predictedlabel = dict()\n",
    "    d_actuallabel = dict()\n",
    "    threshold = v_threshold\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "# 2\tNOT_SARCASM\ttest\tbacking\t0.285714285714\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 4:\n",
    "            continue\n",
    "            \n",
    "        id = words[0].strip()\n",
    "#        if id not in acceptable:\n",
    "#            continue\n",
    "        label = words[3].strip()\n",
    "        if 'non' in label.lower():\n",
    "            d_actuallabel[id] = 'not_sarcasm'\n",
    "        else:\n",
    "            d_actuallabel[id] = 'sarcasm'\n",
    "            \n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "        word1 = words[1].strip()\n",
    "        word2 = words[2].strip()\n",
    "        #val = words[4].strip()\n",
    "        #if words[4] == 'None' or words[4] == '-1':\n",
    "        #    continue\n",
    "        exclude = set(string.punctuation)\n",
    "        word1 = ''.join(ch for ch in word1 if ch not in exclude)\n",
    "        word2 = ''.join(ch for ch in word2 if ch not in exclude)\n",
    "        if word1.strip().lower() in mo.vocab and word2.strip().lower() in mo.vocab:\n",
    "            val = str(mo.similarity(word1,word2))\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if val == -1:\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "            \n",
    "\n",
    "   \n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "        #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "        #f2.write(str(d_minval[id])+'\\t'+str(d_actuallabel[id])+'\\n')\n",
    "    \n",
    "    \n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "    #pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "    \n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "    #nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "    \n",
    "    #print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "    #print('Precision : ' + str(float(tp)/float(tp+fp)))\n",
    "    #print('Recall : '+ str(float(tp)/float(tp+fn)))\n",
    "    #print('Accuracy : '+ str(float(tp+tn)/float(tp+tn+fp+fn)))\n",
    "    #print('F-score : '+ str(fscore))\n",
    "    return (str(threshold)+':'+str(aprecision)+':'+str(arecall)+':'+str(fscore))\n",
    "\n",
    "#acceptable = []\n",
    "#fk = open('/Users/adityaj/work/Sem8/Datasets/BookSnippets/all.stats','r')\n",
    "#for line in fk:\n",
    " #   words = line.split('\\t')\n",
    "  #  fkid = int(words[0])\n",
    "   # fklen = int(words[1])\n",
    "    #if fklen < 120:\n",
    "     #   acceptable.append(fkid)\n",
    "f2 = open('just_output.w','w')\n",
    "for i in np.arange(-0.001,1,0.001):\n",
    "    print(calc_val('/Users/adityaj/DeepLearning/tweet42',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001:0.5616464515524665:0.6446728971962616:0.6003024746273019\n",
      "0.0:0.5612669666452003:0.6443925233644859:0.5999641522578247\n",
      "0.001:0.5638452281420138:0.6446728971962616:0.6015561193090972\n",
      "0.002:0.5628067327513596:0.6442056074766355:0.6007614687610314\n",
      "0.003:0.5614294997243618:0.6437383177570093:0.5997732041119768\n",
      "0.004:0.5657868714223586:0.6443925233644859:0.602536832692592\n",
      "0.005:0.5635899822249139:0.6435514018691588:0.6009223574294963\n",
      "0.006:0.5645431673805752:0.6433644859813085:0.6013821067946298\n",
      "0.007:0.5768184467812116:0.6455140186915889:0.6092358734710639\n",
      "0.008:0.5771599739359499:0.6453271028037383:0.609343003981182\n",
      "0.009:0.5762239514964371:0.6447663551401869:0.6085712801017407\n",
      "0.01:0.5822205609144556:0.6461682242990654:0.612529893670603\n",
      "0.011:0.5825845801970392:0.6460747663551402:0.6126892658882722\n"
     ]
    }
   ],
   "source": [
    "#Rule-based Approach using word2vec similarity and minimum as the measure\n",
    "#This is word2vec similarity for localized approach\n",
    "from nltk.corpus import wordnet as wn \n",
    "import string \n",
    "import numpy as np\n",
    "def calc_val(file,v_threshold): \n",
    "    f = open (file,'r') \n",
    "    d_minval = dict() \n",
    "    d_predictedlabel = dict() \n",
    "    d_actuallabel = dict() \n",
    "    threshold = v_threshold \n",
    "    tp = 0 \n",
    "    tn = 0 \n",
    "    fp = 0 \n",
    "    fn = 0\n",
    "\n",
    "    for line in f:\n",
    "        words = line.strip().split('\\t')\n",
    "        if len(words) != 4:\n",
    "            continue\n",
    "\n",
    "        id = words[0].strip()\n",
    "        label = words[3].strip()\n",
    "        if 'non' in label.lower():\n",
    "            d_actuallabel[id] = 'not_sarcasm'\n",
    "        else:\n",
    "            d_actuallabel[id] = 'sarcasm'\n",
    "\n",
    "        if id not in d_minval.keys():\n",
    "            d_minval[id] = '1'\n",
    "        word1 = words[1].strip()\n",
    "        word2 = words[2].strip()\n",
    "    #val = words[4].strip()\n",
    "    #if words[4] == 'None' or words[4] == '-1':\n",
    "    #    continue\n",
    "        exclude = set(string.punctuation)\n",
    "        word1 = ''.join(ch for ch in word1 if ch not in exclude)\n",
    "        word2 = ''.join(ch for ch in word2 if ch not in exclude)\n",
    "    \n",
    "        if word1.strip().lower() in mo.vocab and word2.strip().lower() in mo.vocab:\n",
    "            val = str(mo.similarity(word1,word2))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if val == -1:\n",
    "            continue\n",
    "        f_val = float(val)\n",
    "        if (float(d_minval[id]) > f_val):\n",
    "            d_minval[id] = str(f_val)\n",
    "\n",
    "\n",
    "\n",
    "    for id in d_minval.keys():\n",
    "        if (float(d_minval[id]) < threshold):\n",
    "            d_predictedlabel[id] = 'sarcasm'\n",
    "        else:\n",
    "            d_predictedlabel[id] = 'not_sarcasm'\n",
    "\n",
    "    #print(d_predictedlabel[id] +' '+d_actuallabel[id])\n",
    "        if d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] == 'sarcasm':\n",
    "            tp += 1\n",
    "        elif d_predictedlabel[id] == 'not_sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            tn += 1\n",
    "        elif d_predictedlabel[id] == 'sarcasm' and d_actuallabel[id] != 'sarcasm':\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    #f2.write(str(d_minval[id])+'\\t'+str(d_actuallabel[id])+'\\n')\n",
    "\n",
    "\n",
    "    pprecision = float(tp)/float(tp+fp)\n",
    "    precall = float(tp)/float(tp+fn)\n",
    "#pfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    nprecision = float(tn)/float(tn+fn)\n",
    "    nrecall = float(tn)/float(tn+fp)\n",
    "#nfscore = float(2*precision*recall)/float(precision+recall)\n",
    "\n",
    "    total = tp + fp + fn + tn\n",
    "    aprecision = float(pprecision)*float(tp+fn)/float(total) + float(nprecision)*float(tn+fp)/float(total)\n",
    "    arecall = float(precall)*float(tp+fn)/float(total) + float(nrecall)*float(tn+fp)/float(total)\n",
    "    fscore = float(2*aprecision*arecall)/float(aprecision+arecall)\n",
    "\n",
    "#print(str(tp)+' '+str(fp)+' '+str(tn)+' '+str(fn))\n",
    "#print('Precision : ' + str(float(tp)/float(tp+fp)))\n",
    "#print('Recall : '+ str(float(tp)/float(tp+fn)))\n",
    "#print('Accuracy : '+ str(float(tp+tn)/float(tp+tn+fp+fn)))\n",
    "#print('F-score : '+ str(fscore))\n",
    "    \n",
    "    return (str(threshold)+':'+str(aprecision)+':'+str(arecall)+':'+str(fscore))\n",
    "#acceptable = []\n",
    "#fk = open('/Users/adityaj/work/Sem8/Datasets/BookSnippets/all.stats','r')\n",
    "#for line in fk:\n",
    "\n",
    "f2 = open('just_output.w','w') \n",
    "for i in np.arange(-0.001,0.012,0.001): \n",
    "    print(calc_val('/Users/adityaj/DeepLearning/tweet42',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
